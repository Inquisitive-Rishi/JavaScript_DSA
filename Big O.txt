There must be a measurement criteria for a code i.e how well a code performs.

What makes a code better?
- faster
- memory efficient
- readibility

comparing codes using performance.now() 

eg-1:
function sumNum(n) {
  let sum = 0;
  for (let i = 1; i <= n; i++) {
    sum+=i;
  }
  return sum;
}

let t1 = performance.now();
sumNum(1_000_000_000)
let t2 = performance.now();

console.log(`Time elapsed: ${(t2-t1)/1000} seconds`);

eg-2:

function sumNum(n) {
  return n*(n+1)/2;
}

let t1 = performance.now();
sumNum(1_000_000_000)
let t2 = performance.now();

console.log(`Time elapsed: ${(t2-t1)/1000} seconds`);

the second example is wayyy faster as the number of operations
don't depend on (n);

eg-2 code has to perform only 3 operations regardless of number of 'n'.
eg-1 code has to perform n no:of operations(loops) + other assignments

so, rather than counting seconds, that are so variable...
we count on number of operation a code has to perform

But, this method is also not that great as counting no:of operations in a code is 
cumbersome and boring:

depending on what we count, the total no:of operations can be as low as 2n or as high as 5n+2.
but regardless of exact no:of operations grows roughly proportionally with n.


BIG O is a way to formalize fuzzy counting.
It allows us to talk how the runtime of an algorithm grows as input grow.


f(n) <-- input,  n <-- output

f(n) could be linear --> (f(n) = n) i.e if the input increases, the runtime increases as well.

f(n) could be quadratic --> (f(n) = n²) i.e if the input increases, 
the runtime increases exponentially

f(n) could be constant --> (f(n) = 1) i.e runtime remains constant regardless of input.


CONSTANTS DO NOT MATTER:

O(2n) --> O(n)   

O(400000) --> O(1)

O(13n²) --> O(n²)  


SMALLER TERMS DO NOT MATTER:

O(n + 10) -->  O(n)

O(10000n + 50) --> O(n)

O(n² + 5n + 8) -->  O(n²)

i.e on a very high level view, smaller terms do not matter.

eg: if we take n = 100 
    O(n² + 5n + 8) ---> O(10_000_000 + (500) + 8) 
// we can see that this 500 value is very very small as compared to 10 million.


RULES OF THUMB: (they don't always work but it's helpful to know)

1. Arithmetic operations are constant.

it will take same amount of time to do 2+2 or 10_000_000 + 10_000_000;

2. Variable assignments are constant.

it will take same amount of time to assign a value to any variable:
eg: x = 2 or x = 10_000_000;

3. Accessing element in an array (using index) or object (using keys) are constant.
    it takes same amount of time to access 1st element or 200th element using index or keys.

4. In case of loops, the complexity is the length of the loop times 
    the complexity of whatever happens inside that loop.
    eg: in case of nested loop it's O(n²)

To summarise:

fastest | O(1)  >  O(n)  >  O(n²) | slowest.

till now, we've been discussing was Time Complexity.


SPACE COMPLEXITY:

In time complexity, we studied how to analyze the 'runtime' of an algorithm
as the size of input increases.

We also use Big O to analyze 'Space Complexity'
This means how much additional memory do we need to allocate in order to run the 
code in our algorithm.

Auxilliary Space Complexity - refers to the space required by the algorithm, 
                              and not including space taken up by the inputs.


When we talk about space complexity, we'll be talking about auxilliary space complexity.


Space Complexity 'rule of thumb':

- most primitives (boolean, numbers, undefined, null) are constant space O(1)
- String requires O(n) space
- Reference types are generally O(n), where n is the length (for arrays) or
  number of keys (for objects).

function sum(arr) {
  let total = 0;
  for (let i = 0; i < arr.length; i++) {
    total+=arr[i];
  }
  return total;
}

here, the length of array has no impact on space, it's just used here to iterate over.
so the space complexity here is O(1).

now,

function doubleAll(arr) {
  let newArr = [];
  for (let i = 0; i < arr.length; i++) {
    newArr.push(arr[i]);
  }
  return newArr;
}

Here, we're creating new array, and the size of the new array keeps on incrasing 
until the iteration is over.

Here, the space increases as n incrases. hence O(n);


LOGARITHMS:

So far we've discussed most common complexities. O(n), O(n²), O(1)

Sometimes, Big O expressions involve more complex mathematical expressions.

SOOOOOO.....! what's logarithm?

Logarithm is the inverse of exponentiation.

Just like division and multiplication are pairs, log and exponentiation are pairs.

log₂(8) = 3 -----> 2**3 = 8;

log₂(value) = exponent ------> 2**exponent = value;

logarithms doesn't just have log with base 2 i.e log₂(8) but can have base 3 or 5 or 10

most common ones are the binary logarithm i.e log₂ ,  log(base10) and loge;

but but but, we are always looking at the 'big picture' here, 

so we omit 2 from log₂ ---> log === log₂


Rule of thumb:

Logarithm(binary) of a number measures the number of times we can divide that number
by 2 before you get a value that's less than or equal to one.

eg:

log₂(8) = 3 how?

8 / 2 --> 8 is divided by two in 3 times 8->4->2->1
in 3rd time it reached 1.

or,

log₂(25) = 5

25 / 2 --> 25->12.5->6.25->3.125->1.5625->0.78125
in 5th time it reached 1 or less than 1 i.e 0.78125.


Logarithmic Complexity is great!

When we plot different Big O notation on a graph.

it goes from O(1) (best) ---> O(n²) (bad)

from best to worst:
O(1) --> O(log n) --> O(n) --> O(nlogn) --> O(n²).


Where is logarithm used?

- Certain searching algorithm has logarithmic time complexity.
- Efficient sorting algorithm involves logarithm.
- Recursion sometimes involves Logarithmic space complexity.


RECAP:

- To analyze the performance of an algorithm, we use BigO.
- Big O can give us a high level understanding of the time or space complexity of algorithm.
- Big O doesn't care about precision, only about general trends (linear? Quadratic? Constant?).
- TIme and space complexity depends only on the algorithm, and not the hardware used to run the algorithm.
- Big O is everywhere.